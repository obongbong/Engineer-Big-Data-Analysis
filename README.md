# Engineer-Big-Data-Analysis
빅데이터 분석 기사 필기 요약 정리

# 빅분기 필기 요약정리

# 📌 1과목

## #빅데이터의 이해

### 1. 빅데이터 개요 및 활용

- 빅데이터 활용 3대 요소 : **인**력, **자**원(데이터), **기**술

- **빅데이터의 3V**
    - Volume(규모)
    - Variety(다양성)
    - Velocity(속도)
    - + Value(가치), Veracity(신뢰성)

- DIKW 피라미드 : 데이터(Data) - 정보(Info) - 지식(Knowledge) - 지혜(Wisdom)

- **암묵지**(나만 아는 지식) - **형식지**(문서, 메뉴얼 등으로 형상화) 상호작용
    
    → ‘**공표연내**’ (공통화 / 표출화 / 연결화 / 내면화)
    

- 데이터베이스의 **특징**
    - 공용 데이터 : 여러 사용자가 공동 이용
    - 통합된 데이터 : 중복x
    - 저장된 데이터
    - 변화하는 데이터 : 새로운 데이터 추가, 수정에도 현재 데이터 유지 ~ *“무결성”*
        
        → ‘**공통저변**’
        

- 빅데이터가 만들어내는 변화
    - 표본 → 전수조사
    - 사전처리 → 사후처리
    - 질 → 양
    - 인과관계 → 상관관계

- 데이터 사이언스의 핵심 구성요소 : Analytics(이론적 지식) / **I**T(프로그래밍적 지식) / **비**즈니스 분석(비즈니스적 능력)
    
    
- 데이터 사이언티스트의 필요역량
    1. **하드 스킬** ~ 이과적 역량 (이론적 지식)
    2. **소프트 스킬** ~ 문과적 역량 (창의력, 분석, 리더십 등)

- **하둡** : 여러 컴퓨터를 하나로 묶어 대용량 데이터를 처리하는 오픈 소스 솔루션

- **데이터 단위**
    
    KB < MB < GB < TB < **P**B < **E**B < **Z**B < **Y**B (Pe - E - Z - Yo)
    
    2의 10승 - 2의 20승 - 2의 30승 …
    

- 빅데이터 조직 및 인력방안
    1. **집중 구조** : 독립 전담 조직 구성
    2. **기능 구조** : 각 현업 부서들에서 직접 분석 (따로 분석 조직이 없음)
    3. **분산 구조** : 분석 조직의 인력을 현업 부서에 배치

### 2. 빅데이터 기술 및 제도

- 빅데이터 플랫폼의 계층 구조
    - **인프라(하위) < 플랫폼 < 소프트웨어 (상위)**

- 딥러닝 < 머신러닝 < 인공지능
    - 약인공지능 : 주어진 조건에서만 동작
    - 강인공지능 : 인간과 동일한 사고가 가능

- 머신러닝의 종류
    - 지도학습 : 정답을 알려줌
    - 비지도학습 : 정답을 알려주지 않음
    - 강화학습 : 보상을 받기 위해 학습

- **전이학습** : 사전에 훈련된 모델을 재사용하는 학습 방식
    - fine-tuning : 이미 학습된 모델을 특정 타겟에 맞게 재조정

- **데이터 3법** : **개**인정보보호법 / **정**보통신망 ~ / **신**용정보 ~
    - 주요 특징
        1. 가명정보의 가념 도입 ~ 가명처리 시 동의 없이 활용 가능
        2. 개인정보보호 거버넌스 체계 효율화
        3. 개인정보처리자 책임 강화 → 사용자의 책임
        4. 개인정보 판단기준 명확화

- 개인정보 비식별 조치 가이드라인 : 사전 검토 → 비식별조치 → 적정성 평가 → 사후 관리

## #데이터분석 계획

### 1. 분석방안수립

- ⭐️분석 대상과 방법
    
    ![스크린샷 2024-04-03 오전 11.27.59.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/a51b2ad5-8122-4731-b13a-457575244d9a/8264c0ca-03ed-40b6-bd40-60e0f86cd106/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-04-03_%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB_11.27.59.png)
    

- 분석 기획 방안
    - 과제 중심적 접근 : 빠르게 해결
    - 장기적 마스터 플랜 : 지속적 분석 원인 해결

- **하향식 접근 방법**
    - 문제가 먼저 주어지고 해결방법을 찾기 위해 진행
        
        문제 탐색 → 문제 정의 → 해결방안 → 타당성 검토
        
        - 문제 탐색 : 비즈니스 모델 캔버스 단순화 (5가지 단순화 - 업무/제품/고객/규제와감사/지원 인프라) ⇒ ‘**지원인프라** **업무** 중 **고객**이 **제품**을 **규제와 감사** 했다’

- **상향식 접근 방법**
    - 문제 정의 자체가 어려울 때

- 분석 방법론의 구성요소 : 전차, 방법, 도구와 기법, 템플릿과 산출물

- 분석 과제에서 고려해야 할 5가지 요소
    - 데이터 크기, 속도, 데이터 복잡도, 분석 복잡도, **정확도/정밀도**
        
        *정확도와 정밀도는 Trade-off 관계
        

- **⭐️ROI 관점**
    - 시급성 : 비즈니스 관점(Value)
    - 난이도 : 투자요소 관점(3V - 다양성, 속도, 규모)
        
        ![스크린샷 2024-04-03 오전 11.57.53.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/a51b2ad5-8122-4731-b13a-457575244d9a/1aa6faa8-a984-40ef-9df8-cf20c1da3d16/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-04-03_%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB_11.57.53.png)
        
        - 시급성 중요 : 3사분면 → 4사분면 → 2사분면
        - 난이도 중요 : 3사분면 → 1사분면 → 2사분면

- 분석 방법론 모델
    1. 폭포수 모델 : Top-Down
    2. 나선형 모델 : 점진적으로 완성, 위험요소 제거에 초점
    3. 프로토타입 모델 : 프로토타입을 우선 개발한 후 보완
    4. 애자일 : 일정 주기를 가지고 프로토타입 끊임없이 수정 → 고객의 니즈 반영

- **KDD** 분석 방법론
    - 데이터 선택 → 전처리 → 변환 → 마이닝 → 결과 평가

- **Crisp-DM** 분석 방법론
    - 업무 이해 → 데이터 이해 → 데이터 준비 → 모델링 → 평가 → 전개
        
        *모델링 단계에선 ‘모델 평가’, 평가 단계에선 ‘모델 적용성 평가’ 수행
        

- **⭐️⭐️⭐️빅데이터 분석 방법론 (순서)**
    - **PPADD**
    
    ![스크린샷 2024-04-03 오후 12.12.15.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/a51b2ad5-8122-4731-b13a-457575244d9a/d1aeca4e-d984-44a4-ae0c-65186619dbc2/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-04-03_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_12.12.15.png)
    
    - **분석기획**
        1. 비즈니스 이해 및 범위 설정 : *구조화된 작업 기술서(SOW)* 작성
        2. 프로젝트 정의 및 계획 수립 : 프로젝트 계획서, 목적, 기대효과, *작업분할구조도(WBS)* 작성
        3. 프로젝트 위험계획 수립 : **회피**(기간 연장, 범위 축소..), **전이**(보험, 사후 보증), **완화**(절감 노력), **수용**(긴급 대책, 조치x, 백업 플랜)

- 데이터 분석 단계
    
    : 분석용 데이터 준비 → 텍스트 분석 → 탐색적 분석(EDA) → 모델링(데이터 분석) → 모델 평가 및 검증
    

- **분석** **거버넌스** 체계 구성요소
    - 조직, 프로세스, 시스템, 데이터, 분석관련 교육 및 마인드 육성체계

- 데이터 분석 수준 진단
    1. 분석 준비도 : 분석업무파악, 분석 인력 및 조직, 기법, 데이터, 문화, 인프라.. 등
    2. 분석 성숙도 : CMMI 모델 기반, 비즈니스 / 조직, 역량 / IT 부문 관점으로 구분
        
        → 도입, 활용, 확산, 최적화 시기별로 각 관점에서의 내용 구분
        

- 데이터 분석 성숙도 모델 → 기업의 도입 적정성 여부 평가 (**도준정확**)
    
    ![스크린샷 2024-04-03 오후 12.18.31.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/a51b2ad5-8122-4731-b13a-457575244d9a/8f3fc136-4af7-4237-a50e-39191baa6a9c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-04-03_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_12.18.31.png)
    

- **데이터 거버넌스**
    - 전사 차원에서 데이터에 대해 표준화된 관리 체계 수립
    - 구성요소 : 원칙, 조직, 프로세스 (**원조프**)

## #데이터 수집 및 저장 계획

### 1. 데이터 수집 및 전환

- **데이터 수집 기술**
    1. **ETL** : 추출 - 변환 - 적재
    2. FTP :  TCP/IP 네트워크에서 컴퓨터 간 파일 교환
    3. API
    4. 스쿱 : RDBMS - 하둡 간 대용량 데이터 전송
    5. 웹 크롤링

- 반정형 데이터는 ‘메타데이터’(설명해주는 데이터)를 포함

- 개인정보 **비식별화**
    - 데이터 마스킹
    - 가명처리
    - 총계처리 (합, 평균.. 등)
    - 범주화 (34세 → 30~40세)

- 프라이버시 보호 모델
    1. k-익명성 : 일정 수준 이상으로 비식별 되도록 함
    2. l-다양성 : 다양성 높임
    3. t-근접성 : 분포를 낮춤

- 데이터 품질 기준 : 정확성, 일관성, 유용성, 접근성, 적시성, 보안성
    - 완전성 : 데이터가 결측치 없이 값을 가지고 있는지
    - 정확성 : 오류가 입력되지 않게 하고, 유효한 내용인지

### 2. 데이터 적재 및 저장

- 분산 파일 시스템
    1. HDFS(하둡 분산파일 시스템)
        - 분산 처리 - 시스템의 과부화 및 병목 현상 해소
    2. 맵리듀스
        - 분산 데이터를 병렬로 처리
        - **Input -> Splitting -> Mapping -> Shuffling -> Reducing**
        - 입력 데이터를 쪼개서 맵핑하고 섞은 후 분류함
    3. GFS (구글 데이터 처리를 위해 설계)

- 데이터베이스
    - 관계형 데이터베이스 : 정형 데이터 처리
        
        *병렬DBMS : 대규모 처리를 위해 일정 단위로 나눠 병렬처리하는 시스템, 빠른 처리가 가능하며 시스템 용량 확장이 용이함 (중복 저장과는 관련 x)
        
    - NoSQL : 비정형 데이터 처리

- DW, DM, Data Lake
    1. **데이터 웨어하우스(DW)** : 정형 데이터 저장 + 주제지향성, 데이터 통합(일관된 형식), 시계열성, 비휘발성
    2. 데이터 마트(DM) : 데이터 웨어하우스의 한 분야, 특정 목적을 위해 사용 
    3. 데이터 레이크 : 다양한 유형의 대량 데이터 저장, 원시(RAW) 및 비정형 데이터 저장, 하둡과 연계

# 📌 2과목

## #데이터 전처리

### 2. 분석 변수 처리

- 변수 선택 방법
    - 전진선택법 : 하나씩 추가
    - 후진제거법 : 모두 있는 상태에서 하나씩 제거
    - 단계별 선택법 : 전진 + 후진

- 차원축소
    
    *차원의 저주 : 데이터 학습 시 차원이 높아질수록 알고리즘 성능이 저하됨
    
    - **주성분 분석 (PCA)** : 상관성 높은 변수의 선형 결합으로 차원 축소 & 새로운 변수 생성 (정방 행렬)
    - **요인 분석** : 요인들을 회전시켜 데이터 내부 잠재 요인을 도출하고 구조를 해석하는 기법 (독립/종속변수 구분x)
        - 직각회전방식 - 요인들 사이 상관관계를 0으로 만들어 서로 독립적인 것으로 가정 (Varimax)
        - 사각회전방식 - 요인들 사이 상관관계를 0이 아닌 값으로 유지 (Promax, Oblimin)
    - **다차원 척도법** : 개체 사이의 군집을 시각적으로 표현하는 방법
    - 특이값 분해 (SVD) : MxN 크기의 비정방 행렬 분해
    - 선형판별분석 & 독립성분분석

- 요약변수 & 파생변수
    - 요약변수 : 수집된 정보를 종합한 변수 (재활용성)
    - 파생변수 : 기존 변수를 기반으로 새로운 변수를 생성한 것. 논리적 타당성 필요

- **수치형 변수 변환**
    1. Z-score 정규화 : 평균 0, 표준편차 1로 변환
        
        <aside>
        💫 (현재 값 - 평균) / 표준편차
        
        </aside>
        
    2. 최소-최대 정규화
        
        <aside>
        💫 (현재값-최소값) / (최대값-최소값)
        
        </aside>
        
    3. 로그 변환

- **범주형 변수 변환**
    1. 레이블 인코딩 : 데이터를 단순 정수로 변환 (0, 1, 2…)
        
        *컴퓨터가 정수를 순서가 아닌 크기로 인식할 수 있음
        
    2. 원-핫 인코딩 : 고유값 해당 칼럼만 1로 표시, 나머지는 0으로 표시 ex) [0, 0, 1]…
    3. 타깃 인코딩 : 타깃 변수를 평균값으로 변환

- 불균형 데이터 처리
    1. 가중치 균형 적용 : 불균형 데이터에 가중치를 줌
        
        +) 비용민감학습 : 소수 클래스에 더 많은 가중치를 줌으로써 예측 정확도 향상 (모든 클래스의 중요도가 동일하지 않은 경우에 사용)
        
    2. 언더샘플링 : 다수 데이터를 일부만 선택
    3. 오버샘플링 : 소수 데이터를 복사해서 수를 늘림
        
        *SMOTE : 가상 직선을 활용해 소수 클래스 데이터를 합성하는 오버샘플링 기법
        

## #데이터 탐색

### 1. 데이터 탐색 기초

- EDA (탐색적 자료 분석)
    - 데이터 의미를 찾기 위한 **통계, 시각화**를 의미 ~ 데이터를 이해하며 의미 있는 관계를 찾아내는 과정
    - 4가지 주제 : 저항성(이상치 영향 적게 받음), 잔차 계산, 자료변수의 재표현, 그래프를 통한 현시성

- 상관분석
    - 피어슨 상관분석 양적 척도 (연속형 변수, 선형관계)
    - 스피어만 상관분석 : 서열 척도 (비선형적 관계 나타냄)

- 첨도와 왜도
    1. **첨도** : 자료 분포가 얼마나 뽀족한지
        - 0 = 정규분포, 값이 클수록 뾰족함 (3을 기준으로 정규분포 형태를 판단하기도 함)
    2. **왜도** : 자료 분포의 비대칭 정도
        - 0보다 크면 오른쪽으로 꼬리 긴 형태 (최빈값 < 중앙값 < 평균값)
        - 0보다 작으면 왼쪽으로 꼬리 긴 형태 (최빈값 > 중앙값 > 평균값)

### 2. 고급 데이터 탐색

- 시공간 데이터 (공간 정보 + 시간 흐름)
    
    #활용 : 시공간 패턴을 통한 예측, 지도를 통한 위치정보, 지리공간의 격자 차트 결합
    

- 다변량 데이터 탐색방법
    
    *다변량 데이터 = 높은 차원의 데이터 ⇒ 차원을 축소해서 분석해야 함
    
    1. 주성분 분석(PCA)
    2. 다차원척도법(MDS) : 데이터간 거리를 보존하여 차원 축소
    3. 로컬선형임베딩(LLE)

## #통계기법 이해

### 1. 기술통계

- 표본 추출 방법
    1. 랜덤 추출법
    2. 계통 추출법 : 번호를 부여해서 일정 간격으로 추출
    3. 집락 추출법 : 여러 군집으로 나눈 뒤 군집을 선택하여 랜덤 추출
        - 군집 내 이질적, 군집 간 동질적
    4. 층화 추출법 : 유사한 요소끼리 층을 묶어서 층별 추출
        - 군집 내 동질적, 군집 간 이질적
    5. 복원 / 비복원 추출

- **확률분포** : 확률변수가 특정 값을 가질 확률을 나타내는 **함수**
    1. ***이산 확률분포*** - 값을 셀 수 있는 분포 (확률질량함수)
        - 이산균등분포 : 모든 곳에서 값 일정
        - 베르누이분포 : 매 시행마다 오직 2가지 결과 뿐
        - 이항분포 : n번의 독립적 베르누의 시행 중 성공할 확률 p를 가지는 분포
        - 기하분포 : 처음 성공할 때까지의 시도횟수를 확률변수로 가지는 분포
        - 다항분포 : 여러 값을 가질 수 있는 확률변수들에 대한 분포
        - 포아송분포 : 단위 공간 내에서 발생할 수 있는 사건의 발생 횟수 표현
        
        ⇒ **베포항항하**
        
    2. ***연속 확률분포*** - 값을 셀 수 없는 분포 (확률밀도함수)
        - 정규분포 - Z검정
        - t분포 : 두 집단의 평균치 차이 비교 - T검정
            - 데이터 개수 **30개 이상**이면 정규성 검정 불필요
            - 이유) 데이터 개수가 많을수록 정규분포랑 유사한 형태가 되기 때문
        - 카이제곱분포 : 두 집단의 동질성 검정 / 단일 집단 모분산에 대한 검정 - 카이제곱검정
        - F분포 : 두 집단 분산의 동일성 검정 - F검정
        - 지수분포

- **표본집단의 표본분포**
    - **표본분포의 평균**은 모집단의 평균과 같음
    - **표본분포의 분산**은 모집단의 분산을 표본 크기로 나눈 것과 같음

- **중심극한정리** : 표본 크기가 충분히 크면(n=30) 모집단 분포에 상관없이 표본분포가 정규분포를 이룸

### 2. 추론통계

- 점추정 : 모집단이 특정 값으로 추정
- 구간추정 : 모집단이 특정 구간으로 추정 (95%, 99%를 가장 많이 사용)

- 모평균의 구간 추정
    - 모집단 분산을 알고 있는 경우
    - 모집단 분산을 모르는 경우 = 정규분포를 사용할 수 없음
        - **자유도가 n-1인 t분포**를 이용
        
        ![참고](https://prod-files-secure.s3.us-west-2.amazonaws.com/a51b2ad5-8122-4731-b13a-457575244d9a/2ce8e495-bd23-48d3-91e0-6c985c20dfea/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-04-03_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.41.10.png)
        
        참고
        

- 가설검정
    
    : 모집단 특성에 대한 주장을 가설로 세우고 표본조사를 통해 가설의 채택여부를 판정
    
    - 귀무가설(H0) : 일반적으로 생각하는 가설
    - 대립가설(H1) : 증명하고자 하는 가설
    - 유의수준(a) : 1종 오류를 범할 확률의 허용 한계
    - 기각역 : 귀무가설이 기각되고 대립가설이 채택되는 검정통계량의 영역
        
        *#귀무가설이 사실인데 거짓이라고 판정 = 1종 오류 (판매자 오류)*
        
        *#귀무가설이 거짓인데 사실이라고 판정 = 2종 오류 (소비자 오류)*
        

- **⭐️가설검정 문제 풀이 방법⭐️**
    1. 귀무가설 / 대립가설 설정
    2. 양측 혹은 단측검정 확인
        - 값이 ‘같지 않다’ → 양측검정 / ‘값이 크다’, ‘값이 작다’ → 단측검정
    3. 일표본 혹은 이표본 확인 (모집단이 하나인지 2개 이상인지)
    4. 검정통계량 계산과 기각역 판단
    5. *t검정인 경우) 단일 / 대응 / 독립표본 확인*
        - 모집단에 대한 평균검정 : 단일표본
        - 동일 모집단에 대한 평균비교 검정 : 대응표본
        - 서로 다른 모집단에 대한 평균비교 검정 : 독립표본

![스크린샷 2024-04-03 오후 4.01.34.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/a51b2ad5-8122-4731-b13a-457575244d9a/286183f0-1d8f-430d-87f8-c4b5579888ce/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-04-03_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_4.01.34.png)

# 📌 3과목

## #분석모형 설계

### 1. 분석 절차 수립

- **과대적합** : 모델이 지나치게 데이터를 학습해 매우 복잡해진 모형
- 과소적합 : 데이터를 충분히 설명하지 못하는 단순한 모델

### 2. 분석 환경 구축

- 분석 도구 선정
    - R : 통계 분석에 특화, 처리속도 느림, 강력한 시각화
    - Python : 간결함, 높은 가독성, R보다 빠른 속도, R보다 약한 시각화

- 데이터 분할 : 과대적합/과소적합 방지 및 데이터 불균형 문제 해결
    - 훈련용(Training) : 모델 학습 - *50%*
    - 검증용(Validation) - *30%*
    - 평가용(Test) : 모델 평가 - *20%*

## #분석기법 적용

### 1. 분석기법

- **회귀분석** : 독립변수가 종속변수에 미치는 영향을 파악
    - 잔차 : 실제값과 예측값의 차이 (오차 = 모집단 기준, 잔차 = 표본집단 기준)
    - 회귀계수 추정방법 : **최소제곱법**
        - 잔차의 제곱합이 최소가 되는 회귀계수와 절편을 구하는 방법
    - 회귀모형 평가 : R-square (0~1 사이 값)
- 선형회귀분석의 가정
    1. 선형성 : 종속-독립변수는 선형관계
    2. 독립성 : 잔차와 독립변수 간 상관관계가 없음
        
        **다중공선성* : 독립변수 서로 간에 강한 상관관계가 나타나는 문제
        
        → 차원 축소, 상관계수 분해를 통해 해결 
        
    3. 정상성(정규성) : 잔차가 정규분포의 특성을 지님
    4. 등분산성 : 잔차의 분산이 고르게 분포
    5. 비상관성 : 잔차들끼리 상관이 없어야 함
- 회귀 모형 변수 선택 방법
    - 전진선택법
    - 후진선택법
    - 단계별 선택법 : 전진+후진 / 변수 추가 시 벌점(AIC, BIC) 고려

- **로지스틱 회귀분석** : 종속변수가 범주형 데이터일 때 활용
    - 오즈 : 성공 확률과 실패 확률의 비
    - 로짓(logit) 변환 : 오즈에 자연로그를 취하는 방법
        
        → 독립변수가 n 증가하면 확률이 e의 n승 만큼 증가
        

- **의사결정나무** : 여러 개 분리 기준으로 최종 분류 값을 찾음
    - 분류에서의 분할 방법 : CHAID, CART(지니지수), C4.5/C5.0(엔트로피지수)
    - 회귀에서의 분할 방법 : CHAID, CART
    - 정지규칙 : 분리를 더 이상 수행하지 않게 함
    - 가지치기 : 일부 가지를 제거하여 과적합 방지

- **인공신경망** : 인간의 뇌 구조를 모방한 퍼셉트론 활용
    - 다중 퍼셉트론 - 입력층~출력층 사이에 1개 이상의 은닉층 보유
        
        *은닉층 수는 사용자가 직접 설정
        
- **활성화 함수** : 인공신경망의 선형성 극복 (XOR 문제 해결)
    - 시그모이드 함수 : 0~1 사이 값을 가지며 로지스틱 회귀분석과 유사
    - 소프트맥스 함수 : 목표 데이터가 다범주인 경우 각 범주에 속할 사후확률을 제공
    - 하이퍼볼릭 탄젠트 함수 : 시그모이드의 중심 값을 0으로 이동 → -1~1 사이 값
    - ReLU 함수 : 기울기 소실문제 극복 → 0 또는 1 둘 중 하나 값
- 인공신경망 과적합 방지방안
    - 규제 - **라쏘(L1)** 규제 : 맨하탄 거리 기반(절대값), **릿지(L2)** 규제 : 유클리드 거리 기반(제곱)
    - 드롭아웃 - 일부 퍼셉트론을 비활성화
    - 조기종료
    - 모델의 복잡도 줄이기 - 은닉층의 퍼셉트론 감소
    - 데이터 증강
    - 배치정규화
- 인공신경망 학습 방법
    - 역전파 알고리즘 : 가중치를 수정하여 오차를 줄임 (출력층 → 입력층)
    - 경사하강법 : 기울기를 낮은 쪽으로 이동시켜 극값(global minimum)에 이를 때까지 반복

- **서포트벡터머신(SVM)** : 마진이 최대가 되는 **초평면**을 찾아 분류
    - 초평면(하이퍼플레인) : 데이터를 구분하는 기준이 되는 경계
    - 서포트벡터 : 클래스를 나누는 초평면과 가까운 위치의 샘플
    - 마진 : 하이퍼플레인과 서포트벡터 사이의 거리
    - **커널함수** : 저차원 데이터를 고차원 데이터로 변경하는 함수
- SVM 유형
    1. 하드마진분류 : 오류 비허용
    2. 소프트마진분류 : 마진 내 어느 정도 오류 허용

- **연관분석** : 항목들 간 조건-결과로 이뤄지는 패턴 발견 기법 (장바구니 분석)
    - 특징
        - 결과가 단순, 분명
        - 품목 수 많을수록 계산이 기하급수적으로 증가
        - Apriori 알고리즘 활용
- 연관분석의 지표 (하단 공식 암기 부분에 기재)
    1. 지지도
    2. 신뢰도
    3. 향상도

- **군집분석** : 비지도 학습, 데이터 간 거리나 유사성을 기준으로 군집 나눔
- 거리측도
    1. 연속형 변수
        - **유클리디안 거리** : 차이의 제곱의 합의 루트
        - **맨하튼 거리** : 차이의 절대값의 합
        - **체비셰프 거리** : 차이의 절대값 중 최댓값
        - 표준화 거리 : 유클리디안 거리를 표준편차로 나눔
        - 민코프스키 거리 : 유클리드/맨하튼 거리를 일반화
        - 마할라노비스 거리 : 표준화 거리에서 변수의 *상관성* 고려
    2. 범주형 변수 - 자카드 유사도, 코사인 유사도
- **계층적** 군집분석
    1. 거리측정 방법 : 최단, 최장, 평균, 중심, 와드
    2. 덴드로그램 (Tree 모양 그래프)
- **비계층적** 군집분석
    1. K평균 군집화
        - 지정된 군집 개수에 따라 평균을 기준으로 중심점 설정 → 중심점 변경 시 군집 변할 수 있음
        - 이상치에 민감 → 이에 대응하기 위해 K-medoids 군집방법 존재
    2. DBSCAN
        - 밀도기반, 군집개수 지정 필요 없음
        - 노이즈 / 이상치에 강함
    3. 기타
        1. 퍼지군집화
        2. EM알고리즘
        3. 자기조직화지도(SOM) : 신경망 활용하여 차원축소(고차원 → 저차원)를 통해 군집화

### 2. 고급 분석기법

- 분할표 : 여러 개 범주형 변수를 기준으로 관측치 기록한 표 (오즈비 계산)

- **다변량 분석**
    
    *⇒ 다변량 데이터들은 분석할 때 차원축소!* 
    
    - 요인분석 : 다수 변수들의 상관관계를 분석하여 소수 요인으로 축약하는 기법
        1. 요인추출방법 : 주성분분석, 공통요인분석
        2. 요인회전

- **시계열 분석** : 시간 흐름에 따라 관찰된 자료 특성을 파악하여 미래 예측
    - **정상성** = 모든 시점에 일정한 평균과 분산을 가져야 함
        
        *차분 : 현 시점의 자료를 이전 값으로 빼는 방법
        
        *자기상관 : 현재 상태가 과거&미래와 밀접한 관련이 있음 = 독립적이지 않음 (시계열 데이터에서의 공분산 기법)
        
    - 백색잡음 : 시계열 모형의 오차항 의미
- 시계열 모형
    1. 자기회귀(AR) 모형 : 자신의 과거 값이 미래를 결정
    2. 이동평균(MA) 모형 : 백색잡음들의 선형결합으로 표현 (관측치에 모두 동일 가중치 부여)
        
        *지수평활법 : MA 종류 중 하나로, 최근 관측치에 더 높은 가중치를 부여 
        
    3. 자기회귀누적이동평균(ARIMA) 모형 = AR + MA
        
        : ARIMA (p, d, q)에서 d는 차분 횟수를 의미
        
- 분해시계열 - 시계열에 영향을 주는 요인을 분리해 해석하는 방법
    1. 추세 요인 : 장기적으로 증가하거나 감소
    2. 계절 요인 : 특정 시기에 나타나는 고정된 주기
    3. 순환(주기, cycle) 요인 : 알려지지 않은 주기, 중장기적
    4. 불규칙 요인 : 설명 불가 요인

- **베이지안 기법**
    - 베이즈 정리 (하단 공식 암기에 기재)
    - 나이브베이즈 분류 = 나이브(독립) + 베이즈 이론

- 인공신경망
    - DNN : 은닉층 2개 이상으로 구성된 인공신경망
    - CNN(합성곱 신경망) : 이미지에서 패턴을 찾음
    - RNN(순환 신경망) : 순차적 데이터(시계열 데이터) 학습에 특화
        - **장기의존성** 문제 ~ 과거 정보가 전달되지 못함
            
            ⇒ **LSTM / GRU** 모델(Reset, Update)로 극복
            

- **오토인코더** : 입력 데이터를 인코더로 압축한 후 디코더로 재구성하는 비지도 학습 신경망

- 텍스트 마이닝
    - 통계적 기반
        1. TDM : 문서에서 등장하는 단어들의 빈도를 행렬로 표현
        2. TF-IDF : 단어 등장 빈도를 특정 문서, 전체 문서에서 비교하는 것
    - 단어 수준 기반
        1. Word2Vec : 거리를 기반으로 하여 벡터로 표현 ~ CBOW, Skip-Gram
        2. FastText : 하나의 단어를 여러 개로 잘라서 벡터로 계산
        3. ELMo : 양방향 언어 모델 적용

- 트랜스포머 : RNN의 느린 속도와 병렬 처리 불가 단점을 개선한 모델 (BERT, GPT)

- **앙상블 분석** : 여러 개의 예측 모형들을 조합 → 전체적인 분산을 감소시켜 성능 향상
    1. 보팅 : 다수결 방식
    2. **배깅** : 복원추출하는 붓스트랩으로 학습 후 보팅으로 결합
    3. **부스팅** : 잘못된 분류 데이터에 큰 가중치를 주는 방법(순차적) ~ 이상치에 민감, 병렬처리 불가
    4. 랜덤포레스트 : 배깅 + 의사결정트리 ⇒ 성능 우수하며 이상치에 강함

- 비모수검정
    - 모집단에 대한 정보가 없을 때, 관측 자료의 분포 가정 불가한 상태일 때
    - 두 관측 간 순위나 차이로 검정
    - 종류 : 부호검정, 순위합검정, 만-휘트니 U검정, 크러스컬-월리스 검정

# 📌 4과목

## #분석모형 평가 및 개선

### 1. 분석모형 평가

- **분류모델 평가지표**
    1. **⭐️혼동행렬(오분류표)**
        - 재현율(Recall) = 민감도(sensitivity) = TP Rate = Hit Rate
        - F-1 Score는 정밀도(Precision)와 재현율(Recall)의 조화평균
        - 특이도(Specificity) : 실제 False인 것 중 맞춘 것
        - 정밀도와 재현율은 Trade-off 관계
    2. **ROC 커브**
        - 가로축 = 1-특이도 (=FP Rate), 세로축 = 민감도
        - 면적이 1에 가까울수록 성능 좋다고 평가
    3. 이익 도표(Lift Table)
        - 불균형 데이터 집합에 사용, 성과 향상도를 각 등급별로 파악할 수 있음
        - 향상도 곡선 : 이익도표를 시각화한 곡선

- **회귀모델 평가지표**
    1. 손실함수(비용함수) : MSE, MAE, RMSE… 등
    2. 결정계수(R square) → 0~1 사이 값

- **군집분석 평가지표**
    
    ⇒ 군집 내 유사, 군집 간 분리
    
    1. **실루엣 계수**
        - 실루엣 지표가 1에 가까울수록 잘 된 군집화 (0.5보다 크면 타당한 것으로 평가)
    2. Dunn Index(DI) : 클수록 군집화 잘 되었다고 평가

- **교차 검증**
    1. 홀드아웃 : 훈련용, 평가용 데이터셋 분리
    2. K-fold 교차검증 : 데이터를 k개 집단으로 구분하여 k-1개 학습, 나머지 1개로 평가
    3. LOOCV : 1개의 데이터로만 평가, 나머지로 학습
    4. 붓스트랩 : 복원추출 (데이터 부족과 불균형 문제 해소)

- **적합도 검정**
    1. Q-Q plot : 데이터 정규성을 시각적으로 파악 (대각선 선을 따라 값들이 분포하면 정규성 만족)
    2. 카이제곱 검정
    3. 샤피로 윌크 검정 : 선형상관관계를 측정하여 검정, p-value 0.05보다 크면 정규성 가정
    4. 콜모고로프-스미르노프 검정 : 누적 분포함수 비교한 연속형 데이터 검정, p-value 0.05보다 크면 정규성 가정

### 2. 분석모형 개선

- 하이퍼 파라미터
    1. 경사하강법 : a가 너무 크면 값이 높은 곳으로 발산, 너무 작으면 오랜 시간이 걸림
    2. Batch Size : 하나의 소그룹에 속하는 데이터 수
        - Epoch - 모든 데이터셋을 학습하는 횟수
        - Iteration - Epoch를 한 번 마치기 위해 필요한 배치 수
    - 하이퍼파라미터 튜닝
        
        : 메뉴얼 서치(경험 또는 감으로 설정), 그리드 서치, 랜덤 서치, 베이지안 최적화 (기존 평가 결과를 활용)
        

- 경사하강법 옵티마이저
    1. 확률적 경사 하강법 : 기울기가 가장 작은 지점에 도달하도록 함
    2. 모멘텀 : 관성 물리법칙 적용, 빠른 최적점 수렴 가능
    3. AdaGrad : 기울기 크기에 따라 학습률을 조정 / 처음엔 크게 → 점차 작게 학습
    4. Adam : 모멘텀+AdaGrad ⇒ 좌우 흔들림이 덜함
        
        ![스크린샷 2024-04-04 오후 5.30.40.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/a51b2ad5-8122-4731-b13a-457575244d9a/bad724a9-d95d-40a6-951c-189334338cad/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-04-04_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_5.30.40.png)
        

## #분석결과 해석 및 활용

### 1. 분석결과 해석

ㄴ 문제푸는 방법 위주 설명

- 주성분 분석 (PCA)
    
    ![스크린샷 2024-04-04 오후 12.46.06.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/a51b2ad5-8122-4731-b13a-457575244d9a/1615a02a-4256-4985-851f-66c24ea0faf8/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-04-04_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_12.46.06.png)
    
    - 스크리플롯 : 주성분 개수 선택 시 도움이 되는 그래프 (팔꿈치 기법 이용)
        
        ⇒ 그래프가 완만해지기 직전 단계 개수로 선택
        

- **⭐️회귀모형의 검정결과**
    1. 모형이 통계적으로 유의미한가 : F통계량, p-value
        - 귀무가설 : ‘모든 회귀계수는 0이다’
    2. 회귀계수들이 유의미한가 : t통계량, p-value
        - 귀무가설 ‘회귀계수는 0이다’
    3. 1번과 2번 모두 귀무가설 기각되면 해당 모델 활용
    4. 모형이 설명력을 갖는가 = 결정계수(R square) 값
        
        ![자유도 = DF](https://prod-files-secure.s3.us-west-2.amazonaws.com/a51b2ad5-8122-4731-b13a-457575244d9a/2858717e-5090-4c53-a792-b8f4f17d0784/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-04-04_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_12.50.36.png)
        
        자유도 = DF
        

### 2. 분석결과 시각화

- 시각화 분류
    1. 시간 시각화 : 막대, 점, 선, 계단식
    2. 공간 시각화 : 등치지역도, 카토그램, 등치선도, 버블 플롯
    3. 관계 시각화 : 산점도, 버블차트, **히트맵,** 히스토그램, 파이플롯, 트리맵
        - 데이터 사이 관계나 분포, 패턴 표현
    4. 비교 시각화 : **히트맵**, 체르노프 페이스, 스타차트, 평행좌표계
        - 여러 변수 간 차이나 유사성 비교

- 인포그래픽 = 정보 + 시각적 형상
    - 패턴 발견보다 일반인에게 설득형 메시지 전달이 목적
    - 유형 : **타임라인**(시간 순서로 나열), **컨셉 맵**(주제-내용 간 연관성)
